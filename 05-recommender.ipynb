{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9867a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import struct\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a4d35",
   "metadata": {},
   "source": [
    "### Vectorización de audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0248dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\" Lee features de la base de datos features.db\"\"\"\n",
    "\n",
    "    features_db_path = \"dataset/features.db\"\n",
    "    genres_db_path = \"dataset/genres.db\"\n",
    "    metadata_db_path = \"dataset/metadata.db\"\n",
    "    tags_db_path = \"dataset/tags.db\"\n",
    "\n",
    "    conn = sqlite3.connect(features_db_path)\n",
    "    features_df = pd.read_sql_query(\"SELECT * FROM features\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    conn = sqlite3.connect(genres_db_path)\n",
    "    genres_df = pd.read_sql_query(\"SELECT * FROM genres\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    conn = sqlite3.connect(metadata_db_path)\n",
    "    metadata_df = pd.read_sql_query(\"SELECT * FROM metadata\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    conn = sqlite3.connect(tags_db_path)\n",
    "    tags_df = pd.read_sql_query(\"SELECT * FROM tags\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "    return features_df, genres_df, metadata_df, tags_df\n",
    "\n",
    "features_df, genres_df, metadata_df, tags_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff3cf1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17783, 1051)\n",
      "(1051,)\n",
      "(17783, 1146)\n",
      "(1146,)\n"
     ]
    }
   ],
   "source": [
    "def decode_tempo(b):\n",
    "    try:\n",
    "        if isinstance(b, float):\n",
    "            return b\n",
    "        if isinstance(b, (bytes, bytearray)) and len(b) == 4:\n",
    "            return struct.unpack('f', b)[0]  # <-- 'f' = float32 (4 bytes)\n",
    "        if isinstance(b, str) and ',' in b:\n",
    "            byte_list = list(map(int, b.split(',')))\n",
    "            b = bytes(byte_list)\n",
    "            return struct.unpack('f', b)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ decode_tempo falló con {repr(b)}: {e}\")\n",
    "    return np.nan\n",
    "\n",
    "def decode_array(b):\n",
    "    try: \n",
    "        return np.frombuffer(b, dtype=np.float32)\n",
    "    except Exception:\n",
    "        return b\n",
    "\n",
    "def concat_full_vector():\n",
    "    \"\"\"Concatena en un solo vector datos de features, genres y tags\"\"\"\n",
    "    df = pd.concat([features_df, genres_df, tags_df], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.drop(columns=['clip_id', 'mp3_path'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df['tempo'] = df['tempo'].apply(decode_tempo)\n",
    "    df['mfcc_mean'] = df['mfcc_mean'].apply(decode_array)\n",
    "    df['chroma_mean'] = df['chroma_mean'].apply(decode_array)\n",
    "    df['stft_mean'] = df['stft_mean'].apply(decode_array)\n",
    "    vectors = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_copy = row.drop(labels=['mfcc_mean', 'tempo', 'stft_mean', 'chroma_mean'], errors = \"ignore\").copy()\n",
    "        other_features = row_copy.values.astype(np.float32)\n",
    "        mfcc = row['mfcc_mean']\n",
    "        tempo = np.array([row['tempo']], dtype=np.float32)\n",
    "        chroma = row['chroma_mean']\n",
    "        stft = row['stft_mean']\n",
    "        full_vector = np.concatenate([stft, mfcc, chroma, tempo, other_features])\n",
    "        vectors.append(full_vector)\n",
    "    return np.stack(vectors)\n",
    "\n",
    "def concat_vector():\n",
    "    \"\"\"\"Concatena en un solo vector datos de features solo\"\"\"\n",
    "    df = features_df.copy()\n",
    "    df.drop(columns=['clip_id', 'mp3_path'], inplace=True)\n",
    "    df.dropna(inplace=True) \n",
    "    df['tempo'] = df['tempo'].apply(decode_tempo)\n",
    "    df['mfcc_mean'] = df['mfcc_mean'].apply(decode_array)\n",
    "    df['chroma_mean'] = df['chroma_mean'].apply(decode_array)\n",
    "    df['stft_mean'] = df['stft_mean'].apply(decode_array)\n",
    "    vectors = []\n",
    "    for _, row in df.iterrows():\n",
    "        mfcc = row['mfcc_mean']\n",
    "        tempo = np.array([row['tempo']], dtype=np.float32)\n",
    "        chroma = row['chroma_mean']\n",
    "        stft = row['stft_mean']\n",
    "        full_vector = np.concatenate([stft, mfcc, chroma, tempo])\n",
    "        vectors.append(full_vector)\n",
    "    return np.stack(vectors)\n",
    "\n",
    "vectors = concat_vector()\n",
    "full_vectors = concat_full_vector()\n",
    "print(vectors.shape)\n",
    "print(vectors[0].shape)\n",
    "print(full_vectors.shape)\n",
    "print(full_vectors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8fdfb",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0ace990",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(vectors)\n",
    "X_full = scaler.fit_transform(full_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63d64bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "X_full_train, X_full_test = train_test_split(X_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa4cfa",
   "metadata": {},
   "source": [
    "### Importación de autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6cb25eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Clase de muestreo para el VAE\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b0a55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"Carga los modelos de autoencodes\"\"\"\n",
    "    custom_objects = {'Sampling': Sampling}\n",
    "    encoder = tf.keras.models.load_model(\"models/encoder.keras\")\n",
    "    encoder_full = tf.keras.models.load_model(\"models/encoder_full.keras\")\n",
    "    vae_encoder = tf.keras.models.load_model(\"models/vae_encoder.keras\", custom_objects=custom_objects)\n",
    "    vae_full_encoder = tf.keras.models.load_model(\"models/vae_full_encoder.keras\", custom_objects=custom_objects)\n",
    "\n",
    "    return encoder, encoder_full, vae_encoder, vae_full_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2922bc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m556/556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step\n",
      "\u001b[1m556/556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step\n",
      "\u001b[1m556/556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step\n",
      "\u001b[1m556/556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step\n"
     ]
    }
   ],
   "source": [
    "def load_encoded_data(vectors, full_vectors):\n",
    "    \"\"\"Carga los datos codificados\"\"\"\n",
    "    encoder, encoder_full, vae_encoder, vae_full_encoder = load_models()\n",
    "    encoded_vectors = encoder.predict(vectors)\n",
    "    encoded_vectors_full = encoder_full.predict(full_vectors)\n",
    "    vae_vectors_data = vae_encoder.predict(vectors)\n",
    "    vae_vectors_data_full = vae_full_encoder.predict(full_vectors)\n",
    "\n",
    "    return encoded_vectors, encoded_vectors_full, vae_vectors_data, vae_vectors_data_full\n",
    "\n",
    "encoded_vectors, encoded_vectors_full, vae_vectors_data, vae_vectors_data_full = load_encoded_data(X, X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6724a6f",
   "metadata": {},
   "source": [
    "### Similitud entre vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5001feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_by_index(index):\n",
    "    \"\"\"Devuelve el clip_id y el mp3_path de la canción en la posición index\"\"\"\n",
    "    conn = sqlite3.connect(\"dataset/features.db\")\n",
    "    features_df = pd.read_sql_query(\"SELECT * FROM features\", conn)\n",
    "    conn.close()\n",
    "    return features_df.iloc[index][['clip_id', 'mp3_path']].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3886734",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f257466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_song_id(path):\n",
    "\t\"\"\"Extrae el identificador de la canción sin los tiempos del fragmento.\"\"\"\n",
    "\treturn re.sub(r\"-\\d+-\\d+\\.mp3$\", \"\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c1a6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_by_fragment_knn(fragment_index, embeddings, fragment_to_song, n_songs=5, search_pool=50, n_jobs=6):\n",
    "    \"\"\"\n",
    "    Devuelve hasta `n_songs` diferentes basados en fragmentos similares.\n",
    "    - fragment_index: índice del fragmento base en `embeddings`.\n",
    "    - embeddings: matriz de embeddings de fragmentos.\n",
    "    - fragment_to_song: lista que mapea fragmento → canción.\n",
    "    - n_songs: cantidad de canciones únicas deseadas.\n",
    "    - search_pool: cuántos vecinos buscar inicialmente (se expandirá si no se alcanzan n_songs).\n",
    "    \"\"\"\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=search_pool, algorithm='ball_tree', n_jobs=n_jobs, metric='minkowski')\n",
    "    knn.fit(embeddings)\n",
    "    distances, indices = knn.kneighbors(embeddings[fragment_index].reshape(1, -1), n_neighbors=search_pool)\n",
    "\n",
    "    base_song_id = fragment_to_song[fragment_index]\n",
    "\n",
    "    seen_songs = OrderedDict()\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        song_id = fragment_to_song[idx]\n",
    "        if song_id == base_song_id:\n",
    "            continue\n",
    "        if song_id not in seen_songs:\n",
    "            seen_songs[song_id] = dist\n",
    "        if len(seen_songs) >= n_songs:\n",
    "            break\n",
    "\n",
    "    return list(seen_songs.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "933b1073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendaciones para el fragmento 0 -> BWV54 - I Aria | American Bach Soloists:\n",
      "  | -BWV54 - III Aria- | American Bach Soloists (distancia: 1.8507)\n",
      "  | Aria_ Diedi il core ad altra Ninfa | Philharmonia Baroque Orchestra (distancia: 2.1467)\n",
      "  | Duet - Wenn kommst du mein Heil | American Bach Soloists (distancia: 2.2835)\n",
      "  | Entree des Pelerins _ Air de furie _ Sarabande (Rameau_ Les Paladins) | Philharmonia Baroque (distancia: 2.2889)\n",
      "  | Duet - Mein Freund ist mein | American Bach Soloists (distancia: 2.4094)\n"
     ]
    }
   ],
   "source": [
    "metadata_df['song_id'] = metadata_df['mp3_path'].apply(_extract_song_id)\n",
    "index = 0\n",
    "song = metadata_df.iloc[index]\n",
    "print(f\"Recomendaciones para el fragmento {index} -> {song['title']} | {song['artist']}:\")\n",
    "\n",
    "for song_id, dist in recommend_songs_by_fragment_knn(0, encoded_vectors, metadata_df['song_id'].values):\n",
    "    row = metadata_df[metadata_df['song_id'] == song_id].iloc[0]\n",
    "    song_id = row['title']\n",
    "    artist = row['artist']\n",
    "    print(f\"  | {song_id} | {artist} (distancia: {dist:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
